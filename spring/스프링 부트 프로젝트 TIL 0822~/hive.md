비구조화 데이터, 빅 데이터인 텍스트, 동영상, 음성을 하둡 분산 처리 시스템으로 저장 및 처리하는데



# 왜 Hive를 사용하는가

- MapReduce 작업을 하기 위해 자바로 코드를 만들어야 한다

- 외부 데이터를 하둡에 넣기 위해 코드 만들기 힘들다

- 데이터 처리 흐름을 직접 만들어야 한다

- 서비스 어플리케이션의 데이터 보관소로 사용하기 원한다

-> 원래 사용하던 db언어 sql과 유사한 hql언어를 사용하여 편리하게 하둡을 이용할 수 있다



# 하둡의 합체시스템 Hive

HDFS를 자유롭게 사용하는 것의 한계

합체 시스템에 Hive말고 Pig도 있다



# Hive란?

하둡에서 동작하는 sql 프로그램을 구현할 수 있는 Hive는  하둡 기반의 데이터 웨어하우스 인프라로 관계형 데이터베이스에 익숙한 개발자에게 인터페이스 제공

sql과 같이 선언적으로 데이터 처리 가능

HDFS나 HBase와 같은 빅데이터 원본을 HQL(HiveQL) 질의 언어를 이용하여 분석

MapReduce 기반의 실행 부분과 데이터가 저장된 공간의 메타데이터 정보, 사용자나 응용 프로그램에서 질의 를 입력 받아 실행시키는 실행 부분으로 구성됨



# 구성요소

- UI
  - 사용자가 쿼리 및 기타 작업을 시스템에 제출하는 사용자 인터페이스
  - CLI, Beeline, JDBC 등
- Driver
  - 쿼리를 입력받고 작업을 처리
  - 사용자 세션을 구현하고, JDBC/ODBC 인터페이스 API 제공
- Compiler
  - 메타 스토어를 참고하여 쿼리 구문을 분석하고 실행계획을 생성
- Metastore
  - 디비, 테이블, 파티션의 정보를 저장
- Execution Engine
  - 컴파일러에 의해 생성된 실행 계획을 실행



![](assets/2022-09-05-22-40-52-image.png)



# Hive 실행 순서

1. 사용자가 제출한 SQL문을 드라이버가 컴파일러에 요청하여 메타스토어의 정보를 이용해 처리에 적합한 형태로 컴파일

2. 컴파일된 SQL을 실행엔진으로 실행

3. 리소스 매니저가 클러스터의 자원을 적절히 활용하여 실행

4. 실행 중 사용하는 원천데이터는 HDFS등의 저장장치를 이용

5. 실행결과를 사용자에게 반환

MapReduce 작업이 필요할 때 Hive는 Java MapReduce 프로그램을 생성하는 것이 아니라 내장된 Generic Mapper와 Reducer 모듈을 사용하고 Job Plan이라는 XML파일을 적용해 MapReduce Job을 생성해서 Job Tracker와 통신




















